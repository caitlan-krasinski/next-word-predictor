{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Next Word Predictor</h1>\n",
    "<p>Data collection and cleaning done in data preprocessing notebook</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"gullivers_travels.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i desired the secretary to present my humble d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>but not to detract from a nation to which dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he consented and i immediately stripped mysel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i then desired the governor to call up descart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i formerly hinted to you something of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paragraphs\n",
       "0  i desired the secretary to present my humble d...\n",
       "1   but not to detract from a nation to which dur...\n",
       "2   he consented and i immediately stripped mysel...\n",
       "3  i then desired the governor to call up descart...\n",
       "4   when i formerly hinted to you something of th..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = list(data['paragraphs'])\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' but not to detract from a nation to which during my life i shall acknowledge myself extremely obliged it must be allowed that whatever this famous tower wants in height is amply made up in beauty and strength for the walls are near a hundred feet thick built of hewn stone whereof each is about forty feet square and adorned on all sides with statues of gods and emperors cut in marble larger than the life placed in their several niches'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Collect unique words in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['men',\n",
       " 'remarkable',\n",
       " 'collected',\n",
       " 'objects',\n",
       " 'seats',\n",
       " 'wit',\n",
       " 'informations',\n",
       " 'integrity',\n",
       " 'rogues',\n",
       " 'temperance',\n",
       " 'uneasiness',\n",
       " 'sufficient',\n",
       " 'travelled',\n",
       " 'chinese',\n",
       " 'canal',\n",
       " 'upright',\n",
       " 'reflect',\n",
       " 'fairer',\n",
       " 'shortening',\n",
       " 'earnestness']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for i in rows:\n",
    "    row = i.replace('\\n','').split()\n",
    "    for word in row:\n",
    "        words.append(word)\n",
    "\n",
    "        \n",
    "words = set(words)\n",
    "words = list(words)\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(data=words, index=None, columns=['words'])\n",
    "words_df.to_csv('words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8147"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target and feature sets\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for j in rows:\n",
    "    sentence = j.split()\n",
    "    last_word = sentence[-1] #pluck last word\n",
    "    sentence.pop() #remove last word\n",
    "    sentence = \" \".join(sentence) \n",
    "    x.append(sentence) #add sentence, minus last word to x\n",
    "\n",
    "    encoded_word = [0]*(len(words)) \n",
    "    encoded_word[words.index(last_word)]=1 #encode last word\n",
    "    y.append(encoded_word)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997, 19997)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25, random_state=55)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but hurried on by the precipitancy of youth and having his imperial majestys license to pay my attendance upon the emperor of blefuscu i took this opportunity before the three days were elapsed to send a letter to my friend the secretary signifying my resolution of setting out that morning for blefuscu pursuant to the leave i had got and without waiting for an answer i went to that side of the island where our fleet'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x189d09310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x189d09310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x18960b280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x18960b280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x188424af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x188424af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "xtrain = embed(xtrain) #embed\n",
    "xtest = embed(xtest) #embed\n",
    "#covert to numpy arrays/matrices\n",
    "xtrain = xtrain.numpy()\n",
    "xtest = xtest.numpy()\n",
    "ytest = np.array(ytest) \n",
    "ytrain = np.array(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14997, 512), (14997, 8147))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04899583, -0.03114763,  0.00227157,  0.00442289, -0.05547805,\n",
       "        0.06363542,  0.05368468,  0.01622464,  0.06673943,  0.04664867,\n",
       "       -0.04496281, -0.05015457,  0.02276505,  0.0443054 , -0.08912884,\n",
       "       -0.05177389, -0.0866108 , -0.06640136,  0.01699785, -0.06158642,\n",
       "        0.06076851,  0.00676135, -0.07722596, -0.02966137,  0.02902962,\n",
       "       -0.08251665, -0.06471777, -0.03177203, -0.0776021 ,  0.03117883,\n",
       "        0.00563257,  0.0669057 , -0.05106832, -0.04172863, -0.0039697 ,\n",
       "        0.04440599,  0.01250168, -0.0678933 , -0.03196873, -0.04358714,\n",
       "       -0.00074472,  0.08387917,  0.03252276,  0.02008015,  0.00449204,\n",
       "        0.03164991, -0.03134472,  0.01680997, -0.05335777, -0.07710892,\n",
       "       -0.03624308,  0.01026028,  0.02407982,  0.02771026, -0.02083948,\n",
       "       -0.04527139,  0.00093273,  0.05376001, -0.0222789 ,  0.00753828,\n",
       "        0.04776055,  0.04062064, -0.02747052, -0.01985496, -0.06104327,\n",
       "       -0.0201566 , -0.01087866, -0.05189257,  0.04879563, -0.06092855,\n",
       "       -0.08427158, -0.06799278, -0.0108852 ,  0.0091977 ,  0.04607882,\n",
       "        0.04473007, -0.03332759,  0.02740441,  0.0577737 ,  0.03277454,\n",
       "       -0.06017099,  0.02105167,  0.00964491,  0.00505891,  0.07699244,\n",
       "        0.04571773, -0.02548244,  0.02804356, -0.03780107,  0.02216397,\n",
       "        0.01236558,  0.04076891, -0.01326416,  0.05760483,  0.08666527,\n",
       "        0.08414108,  0.04808943, -0.05943836,  0.07079591,  0.03086243,\n",
       "       -0.01010766, -0.00528699,  0.02456877, -0.02315593, -0.05476728,\n",
       "       -0.0654945 ,  0.0545846 ,  0.01387004, -0.06794057, -0.0298395 ,\n",
       "        0.02538532, -0.03243379,  0.05529311, -0.00511949, -0.06972084,\n",
       "       -0.00830767, -0.05209856, -0.08377071, -0.02514898,  0.05486669,\n",
       "        0.04996857, -0.06251153,  0.03621025,  0.07156127,  0.0709805 ,\n",
       "       -0.00887425,  0.02492167, -0.02550014,  0.00539706,  0.02661414,\n",
       "       -0.03454807,  0.08410709,  0.06061065, -0.05677079,  0.05283556,\n",
       "       -0.0112884 , -0.020331  , -0.07811604, -0.05412626, -0.06999574,\n",
       "       -0.04025392,  0.06017803, -0.0122093 ,  0.06726707, -0.07960331,\n",
       "        0.02000011, -0.05039341, -0.0537676 , -0.0599868 ,  0.00377673,\n",
       "       -0.02683993,  0.00601421,  0.01489218,  0.01357557,  0.0058345 ,\n",
       "       -0.00759458,  0.05284766,  0.05587811, -0.05427999,  0.00444046,\n",
       "        0.05647092,  0.04958119, -0.05569021,  0.04353659, -0.03078291,\n",
       "        0.01726441, -0.01197942,  0.02286029,  0.04124937, -0.00369608,\n",
       "        0.0321534 , -0.00919241, -0.04390094, -0.05720055, -0.00332186,\n",
       "       -0.04895815, -0.01703255,  0.01199537,  0.059913  , -0.00406216,\n",
       "        0.07183058,  0.03140022, -0.04793453,  0.02248708, -0.08641867,\n",
       "       -0.00638715, -0.08370369,  0.01357521, -0.05594248, -0.03354897,\n",
       "        0.02088712, -0.04778903,  0.04283521,  0.02055958, -0.05969692,\n",
       "        0.03227017,  0.05493168, -0.00407938, -0.01911102, -0.00293189,\n",
       "       -0.02479493,  0.00039396,  0.06035484,  0.00072191,  0.00772161,\n",
       "       -0.07053317,  0.05520339, -0.0164302 ,  0.05890632,  0.03087371,\n",
       "       -0.02436579, -0.02460052, -0.04331112,  0.04871712, -0.01041263,\n",
       "        0.0269092 , -0.06625516, -0.01795004,  0.06456012,  0.04493243,\n",
       "       -0.00510483,  0.05940416, -0.0186532 , -0.01356684,  0.00236923,\n",
       "       -0.035214  ,  0.03930823, -0.03244875,  0.02068605,  0.00553545,\n",
       "       -0.03880155,  0.02052826, -0.00036134, -0.08547269,  0.05883506,\n",
       "        0.00557565,  0.02181095,  0.0054715 ,  0.07488503, -0.01908359,\n",
       "        0.00892533,  0.07390314, -0.00718376, -0.0414887 , -0.04206381,\n",
       "       -0.03129097,  0.08869812, -0.07300739, -0.03482149, -0.07040677,\n",
       "        0.06250148,  0.04065423,  0.05271522,  0.01334041,  0.03773227,\n",
       "       -0.04259561, -0.00675499, -0.01478154,  0.04759628,  0.00475472,\n",
       "        0.04631536,  0.07823372,  0.04769931,  0.00526338, -0.00580477,\n",
       "        0.0218734 , -0.0026823 ,  0.02230122, -0.04998663, -0.02773032,\n",
       "       -0.018102  ,  0.03953959,  0.00969688,  0.05094574,  0.05140797,\n",
       "        0.00180221,  0.00850008,  0.04283718,  0.02824341,  0.03116739,\n",
       "        0.01154672, -0.0590543 , -0.03593587,  0.05325935,  0.01196823,\n",
       "        0.04893238, -0.01050151,  0.02942614, -0.03771227,  0.05495528,\n",
       "       -0.0124492 , -0.05474968,  0.07153795, -0.07558906,  0.00561416,\n",
       "       -0.0222311 ,  0.00688506,  0.02545406, -0.0053706 ,  0.05043935,\n",
       "       -0.04673323, -0.01245769, -0.01319502,  0.00804343,  0.02831175,\n",
       "        0.01480608,  0.06210843, -0.03679758, -0.06352566, -0.02356795,\n",
       "        0.02180415, -0.03994511,  0.05091387,  0.05111966, -0.07100595,\n",
       "        0.04706606, -0.06737798,  0.04796052, -0.04936098,  0.06710579,\n",
       "       -0.00655067, -0.00658333, -0.00490985,  0.08891937, -0.05795531,\n",
       "       -0.03033477, -0.0491666 ,  0.06186639,  0.01451134,  0.04170483,\n",
       "       -0.02746897,  0.06569432, -0.04712679,  0.03148071,  0.04715638,\n",
       "       -0.08839339, -0.02124428,  0.02253148, -0.05224048, -0.00430665,\n",
       "       -0.0146746 , -0.02084224,  0.07082809,  0.01809564,  0.03163138,\n",
       "       -0.08913163,  0.01740437, -0.05984456, -0.03848604, -0.02573137,\n",
       "        0.01372122, -0.08469832,  0.01563714, -0.0096533 ,  0.04376557,\n",
       "       -0.06475698, -0.07634596,  0.05787927, -0.03050808,  0.03090281,\n",
       "       -0.00449383, -0.04833543,  0.00177901, -0.03304124,  0.0205477 ,\n",
       "       -0.04964312,  0.05471028,  0.02498979, -0.05085394,  0.03151802,\n",
       "        0.02610584, -0.06948923,  0.00356831, -0.05059792, -0.01107351,\n",
       "       -0.04640574, -0.05348647, -0.06047352, -0.0298733 ,  0.08037426,\n",
       "       -0.04024256, -0.03493304,  0.03635301,  0.04632794,  0.04457634,\n",
       "       -0.05638564, -0.05607022,  0.01376912, -0.00251246,  0.04656239,\n",
       "        0.03064064,  0.00593342,  0.06975619,  0.01848958, -0.02782975,\n",
       "        0.00145468, -0.02739923, -0.00606588,  0.01312124, -0.08207805,\n",
       "        0.0609257 , -0.06363657, -0.06279302, -0.06380682,  0.01278814,\n",
       "        0.02969974, -0.03140412,  0.01870434, -0.01794945,  0.04700603,\n",
       "        0.02121067,  0.04922535, -0.03535203, -0.01347856, -0.06842707,\n",
       "       -0.01259753, -0.00016231, -0.05906828,  0.03099999, -0.04987826,\n",
       "       -0.01456112, -0.08576156, -0.05177895,  0.03016043, -0.05725987,\n",
       "       -0.02169823, -0.0158997 ,  0.00343546,  0.06490458,  0.01088297,\n",
       "        0.00492913,  0.03434629, -0.0255072 ,  0.08819406,  0.0744081 ,\n",
       "        0.00717788,  0.0243617 ,  0.03474875, -0.03425744, -0.04306282,\n",
       "       -0.05393228, -0.07240193, -0.01222989, -0.0147783 , -0.00970313,\n",
       "       -0.0689833 ,  0.0314918 ,  0.05017335, -0.01197399, -0.00498298,\n",
       "        0.02481577,  0.00154865,  0.0604845 ,  0.01692625, -0.04730073,\n",
       "        0.03940545, -0.08186676, -0.01227727, -0.02895341, -0.07052369,\n",
       "       -0.01521407,  0.07132307,  0.04550143, -0.03151648,  0.03355701,\n",
       "        0.06659938, -0.04243432, -0.05301392,  0.01868429,  0.02382098,\n",
       "        0.0394427 ,  0.00691869, -0.00111429,  0.04797586,  0.03604995,\n",
       "       -0.06526687,  0.07650163,  0.0345125 ,  0.00042643, -0.02533031,\n",
       "       -0.06437527, -0.02525454, -0.05194146, -0.01645977, -0.00544854,\n",
       "       -0.00102048, -0.04540896, -0.05146953, -0.08905832, -0.06064344,\n",
       "        0.04889009,  0.00507108,  0.07128656, -0.00465198, -0.04335939,\n",
       "       -0.06578606,  0.04681014,  0.03787075,  0.01722642, -0.03214692,\n",
       "       -0.03827484, -0.07515625, -0.04586757,  0.03205077,  0.01751772,\n",
       "       -0.05600877, -0.00927325, -0.0136694 , -0.06853665,  0.03145324,\n",
       "       -0.0200456 , -0.08095091], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8147)              4179411   \n",
      "=================================================================\n",
      "Total params: 4,442,067\n",
      "Trainable params: 4,442,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=[512], activation = 'relu'))\n",
    "model.add(Dense(units=len(words), activation = 'sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "43/43 [==============================] - 6s 133ms/step - loss: 8.6460 - acc: 0.0228 - val_loss: 6.9886 - val_acc: 0.0232\n",
      "Epoch 2/30\n",
      "43/43 [==============================] - 5s 111ms/step - loss: 6.7427 - acc: 0.0250 - val_loss: 6.4615 - val_acc: 0.0238\n",
      "Epoch 3/30\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 6.1467 - acc: 0.0362 - val_loss: 5.7361 - val_acc: 0.0468\n",
      "Epoch 4/30\n",
      "43/43 [==============================] - 5s 109ms/step - loss: 5.2791 - acc: 0.0926 - val_loss: 4.7381 - val_acc: 0.1862\n",
      "Epoch 5/30\n",
      "43/43 [==============================] - 5s 109ms/step - loss: 4.1212 - acc: 0.3121 - val_loss: 3.5549 - val_acc: 0.4060\n",
      "Epoch 6/30\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 2.8909 - acc: 0.5558 - val_loss: 2.5560 - val_acc: 0.5954\n",
      "Epoch 7/30\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 1.9676 - acc: 0.7060 - val_loss: 1.8819 - val_acc: 0.6912\n",
      "Epoch 8/30\n",
      "43/43 [==============================] - 5s 122ms/step - loss: 1.3908 - acc: 0.7893 - val_loss: 1.4452 - val_acc: 0.7606\n",
      "Epoch 9/30\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 1.0373 - acc: 0.8422 - val_loss: 1.1667 - val_acc: 0.8034\n",
      "Epoch 10/30\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 0.7984 - acc: 0.8774 - val_loss: 0.9570 - val_acc: 0.8450\n",
      "Epoch 11/30\n",
      "43/43 [==============================] - 5s 110ms/step - loss: 0.6305 - acc: 0.9062 - val_loss: 0.7964 - val_acc: 0.8640\n",
      "Epoch 12/30\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 0.5155 - acc: 0.9235 - val_loss: 0.6784 - val_acc: 0.8906\n",
      "Epoch 13/30\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 0.4334 - acc: 0.9358 - val_loss: 0.5755 - val_acc: 0.9074\n",
      "Epoch 14/30\n",
      "43/43 [==============================] - 5s 128ms/step - loss: 0.3548 - acc: 0.9492 - val_loss: 0.5014 - val_acc: 0.9226\n",
      "Epoch 15/30\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 0.2976 - acc: 0.9588 - val_loss: 0.4351 - val_acc: 0.9340\n",
      "Epoch 16/30\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.2554 - acc: 0.9649 - val_loss: 0.3827 - val_acc: 0.9476\n",
      "Epoch 17/30\n",
      "43/43 [==============================] - 6s 137ms/step - loss: 0.2196 - acc: 0.9714 - val_loss: 0.3405 - val_acc: 0.9510\n",
      "Epoch 18/30\n",
      "43/43 [==============================] - 5s 122ms/step - loss: 0.1833 - acc: 0.9782 - val_loss: 0.3044 - val_acc: 0.9600\n",
      "Epoch 19/30\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.1604 - acc: 0.9812 - val_loss: 0.2725 - val_acc: 0.9634\n",
      "Epoch 20/30\n",
      "43/43 [==============================] - 5s 112ms/step - loss: 0.1418 - acc: 0.9854 - val_loss: 0.2465 - val_acc: 0.9738\n",
      "Epoch 21/30\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.1260 - acc: 0.9883 - val_loss: 0.2226 - val_acc: 0.9762\n",
      "Epoch 22/30\n",
      "43/43 [==============================] - 6s 149ms/step - loss: 0.1074 - acc: 0.9902 - val_loss: 0.2042 - val_acc: 0.9808\n",
      "Epoch 23/30\n",
      "43/43 [==============================] - 6s 131ms/step - loss: 0.0978 - acc: 0.9919 - val_loss: 0.1880 - val_acc: 0.9830\n",
      "Epoch 24/30\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 0.0853 - acc: 0.9927 - val_loss: 0.1729 - val_acc: 0.9842\n",
      "Epoch 25/30\n",
      "43/43 [==============================] - 5s 111ms/step - loss: 0.0774 - acc: 0.9932 - val_loss: 0.1610 - val_acc: 0.9860\n",
      "Epoch 26/30\n",
      "43/43 [==============================] - 4s 104ms/step - loss: 0.0681 - acc: 0.9946 - val_loss: 0.1518 - val_acc: 0.9864\n",
      "Epoch 27/30\n",
      "43/43 [==============================] - 5s 110ms/step - loss: 0.0605 - acc: 0.9952 - val_loss: 0.1404 - val_acc: 0.9894\n",
      "Epoch 28/30\n",
      "43/43 [==============================] - 5s 110ms/step - loss: 0.0588 - acc: 0.9944 - val_loss: 0.1330 - val_acc: 0.9902\n",
      "Epoch 29/30\n",
      "43/43 [==============================] - 5s 125ms/step - loss: 0.0509 - acc: 0.9957 - val_loss: 0.1264 - val_acc: 0.9908\n",
      "Epoch 30/30\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 0.0484 - acc: 0.9962 - val_loss: 0.1211 - val_acc: 0.9906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18976cd30>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain, ytrain, batch_size=350, shuffle=True, epochs=30, validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Trained model predictions</h2>\n",
    "<p>Model was trained on Gulliver's Travels, and therefore is biased to the language used in the novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(given_words):\n",
    "    prediction = model.predict(x=embed([given_words]).numpy())\n",
    "    index = np.argmax(prediction[-1])\n",
    "    pred = words[index]\n",
    "    given_words += ' ' + words[index]\n",
    "      \n",
    "    print(given_words, '\\nnext word:', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he was on the island \n",
      "next word: island\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "single_text = \"he was on the\"\n",
    "\n",
    "next_word(single_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"next_word_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('next_word_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
